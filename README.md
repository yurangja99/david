# <p align="center"> DAViD: Modeling Dynamic Affordance of 3D Objects Using Pre-trained Video Diffusion Models (ICCV 2025)</p>

## [Project Page](https://snuvclab.github.io/david/) &nbsp;|&nbsp; [Paper](https://arxiv.org/pdf/2501.08333) 

![demo.png](./assets/teaser.png)

This is the official code for the paper "DAViD: Modeling Dynamic Affordance of 3D Objects Using Pre-trained Video Diffusion Models".

## Installation

To setup the environment for running DAViD, please refer to the instructions provided <a href="INSTALL.md">here</a>.


## Datasets
```
david
  ├ animation
  ├ ...
  ├ data
  │  ├ InterAct
  │  │  ├ ...
  │  │  ├ largetable_two_hand_carry
  │  │  │  ├ sub2_largetable_003.pt
  │  │  │  ├ sub2_largetable_005.pt
  │  │  │  └ ...
  │  │  └ ...
  │  ├ InterActObjects
  │  │  ├ objects
  │  │  │  ├ clothesstand
  │  │  │  │  ├ clothesstand.obj
  │  │  │  │  └ sample_points.npy
  │  │  │  └ floorlamp
  │  │  │     ├ floorlamp.obj
  │  │  │     └ sample_points.npy
  │  │  ├ clothesstand.urdf
  │  │  ├ floorlamp.urdf
  │  │  └ ...
  │  └ omomo_text_anno_json_data
  │     ├ sub2_largetable_003.json
  │     ├ sub2_largetable_005.json
  │     └ ...
  ├ ...    
  └ imports
     ├ ...
     ├ mdm
     │  ├ body_models
     │  │  ├ dmpls
     │  │  ├ smpl
     │  │  ├ smplh
     │  │  └ smplx
     │  ├ checkpoints
     │  │  ├ humanml_enc_512_50steps
     │  │  │  ├ args.json
     │  │  │  ├ model000750000.pt
     │  │  │  └ opt000750000.pt
     │  │  └ t2m
     │  └ dataset
     │     ├ HumanML3D
     │     ├ humanml_opt.txt
     │     ├ kit_opt.txt
     │     └ t2m_train.npy
     └ ...
```

## Quick Start

### 2D HOI Image Generation

To generate 2D HOI Images of given 3D object (in this case, barbell), use following command.

```shell
bash scripts/generate_2d_hoi_images.sh --dataset "ComAsset" --category "barbell" --device 0 --skip_done
```

<table>
  <tr>
    <td align="center" width="33%">
      <img src="assets/render.png"><br>
      <sub>Rendered Image</sub>
    </td>
    <td align="center" width="33%">
      <img src="assets/canny.png"><br>
      <sub>Canny Edges</sub>
    </td>
    <td align="center" width="33%">
      <img src="assets/2dhoi.png"><br>
      <sub>2D HOI Image</sub>
    </td>
  </tr>
</table>

### Image-to-Video

We leverage commercial image-to-video diffusion model [Kling AI](https://www.klingai.com/) to make 2D HOI videos from 2D HOI images.
Specifically, we use [imgur](https://imgur.com/) and [PiAPI](https://piapi.ai/docs) for uploading image and calling API for Kling AI. Check out `scripts/videos/get.sh`, `scripts/videos/post_i2v.sh` and setup your `X-API-key` of your PiAPI account. Also check out `constants/videos.py` and setup your client id of your imgur account. Note that you need paid version of Kling AI for directly follow our setting.

```shell
CUDA_VISIBLE_DEVICES=0 python src/generation/generate_videos.py --dataset "ComAsset" --category "barbell" --skip_done
```

Otherwise, you can use open-source image-to-video models such as [Wan2.1](https://github.com/Wan-Video/Wan2.1), but we haven't tested it yet.

### 4D HOI Sample Generation

To generate 4D HOI Samples from the generated 2D HOI Images (of the given 3D object, barbell), use following command.

```shell
bash scripts/generate_4d_hoi_samples.sh --dataset "ComAsset" --category "barbell" --device 0 --skip_done
```

<table>
  <tr>
    <td align="center" width="33%">
      <img src="assets/2dhoi.png"><br>
      <sub>2D HOI Image</sub>
    </td>
    <td align="center" width="33%">
      <img src="assets/2dhoivid.gif"><br>
      <sub>2D HOI Video</sub>
    </td>
    <td align="center" width="33%">
      <img src="assets/4dhoi_incam.gif"><br>
      <sub>4D HOI Sample (Camera View)</sub>
    </td>
  </tr>
</table>


### Visualization

To visualize generated 4D HOI Samples, use following command. 

```shell
blenderproc debug src/visualization/visualize_4d_hoi_sample.py --dataset "ComAsset" --category "barbell" --idx 0
```

![4dhoi.gif](./assets/4dhoi.gif)

### Preprocess FullBodyManip Dataset

Refer to [scripts/preprocess_interact.sh](./scripts/preprocess_interact.sh)

```shell
bash scripts/preprocess_interact.sh
```

### Train LoRA for MDM

To train LoRA for MDM (of the given 3D object, barbell), use following command.

Refer to [scripts/process_mdm.sh](./scripts/process_mdm.sh) and [scripts/train_lora.sh](./scripts/train_lora.sh)

```shell
bash scripts/process_mdm.sh
bash scripts/train_lora.sh
```

### Train Object Motion Diffusion Model

To train Object Motion Diffusion Model (of the given 3D object, barbell), use following command.

Refer to [scripts/process_omdm.sh](./scripts/process_omdm.sh) and [scripts/train_omdm.sh](./scripts/train_omdm.sh)

```shell
bash scripts/process_omdm.sh
bash scripts/train_omdm.sh
```

### Sample Human Motion (Inference)

Refer to [scripts/inference_mdm.sh](./scripts/inference_mdm.sh)

```shell
bash scripts/inference_mdm.sh
```

### Sample Object Motion (Inference)

Refer to [scripts/inference_omdm.sh](./scripts/inference_omdm.sh)

```shell
bash scripts/inference_omdm.sh
```

### Visualize DAViD Output (Inference)

```shell
blenderproc debug src/visualization/visualize_david_output.py --custom-blender-path ../../datasets/blender-3.6.3-linux-x64 --dataset "FullBodyManip" --category "largetable" --idx 0
```

## Regarding Code Release
- We are keep updating the code (including dataset and environment setup)!
- [2025/07/10] Initial skeleton code release!


## Citation
```bibtex
@misc{david,
      title={DAViD: Modeling Dynamic Affordance of 3D Objects Using Pre-trained Video Diffusion Models}, 
      author={Hyeonwoo Kim and Sangwon Baik and Hanbyul Joo},
      year={2025},
      eprint={2501.08333},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2501.08333}, 
}
```